rename(date = datetime)
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2012-2015.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2015-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years, aes(displ, hwy)) + geom_point()
ggplot(all_years, aes(x=date, y=cnt)) + geom_point()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2014-2015.csv")
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
ggplot(all_years)
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
test <- bikes %>%
filter(date >= ymd("2012-07-01"))
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Model using the augmented features
fit_lm <- lm(cnt ~ ., data = train_augmented)
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
# RMSE
sqrt(mean(fit_lm$residuals^2))
test_augmented <- test %>%
tk_augment_timeseries_signature()
test_augmented
yhat_test <- predict(fit_lm, newdata = test_augmented)
pred_test <- test %>%
add_column(yhat = yhat_test) %>%
mutate(.resid = cnt - yhat)
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
train
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
summary(fit_lm)
test_augmented
all_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
mutate(created_at = parse_date_time(created_at, "a b! d! H!:M!:S! z!* Y!")) %>%
tbl_df()
ibrary(mlr)
library(OOBCurve)
library(tidyverse)
library(lubridate)
library(dplyr)
library(purrr)
library(ggplot2)
library(scales)
library(stringr)
library(tidytext)
library(glmnet)
library(tm)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(data.table)
library(ROCR)
library(leaps)
library(xgboost)
### Data importation
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
all_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
mutate(created_at = parse_date_time(created_at, "a b! d! H!:M!:S! z!* Y!")) %>%
tbl_df()
View(all_tweets)
### Restriction to Twitter data and definition of iPhone/Android fields
tweets <- all_tweets %>%
select(id_str, source, text, created_at) %>%
filter(source %in% c("Twitter for iPhone", "Twitter for Android")) %>%
mutate(source = ifelse(source=="Twitter for iPhone", "iPhone", source)) %>%
mutate(source = ifelse(source=="Twitter for Android", "Android", source))
### Data importation
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
all_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
mutate(created_at = parse_date_time(created_at, "a b! d! H!:M!:S! z!* Y!")) %>%
tbl_df()
### Restriction to Twitter data and definition of iPhone/Android fields
tweets <- all_tweets %>%
select(id_str, source, text, created_at) %>%
filter(source %in% c("Twitter for iPhone", "Twitter for Android")) %>%
mutate(source = ifelse(source=="Twitter for iPhone", "iPhone", source)) %>%
mutate(source = ifelse(source=="Twitter for Android", "Android", source))
### Descriptive plots at the aggregate level
tweets %>% filter(year(with_tz(created_at, "EST"))>2014, year(with_tz(created_at, "EST"))<2017) %>%
count(source, hour = hour(with_tz(created_at, "EST"))) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line(lwd=2) +
scale_y_continuous(labels = percent_format()) +
theme(title=element_text(size=18),axis.title=element_text(size=18), axis.text=element_text(size=18),legend.text=element_text(size=18)) +
labs(title="Proportion of tweets by time of day, per source",
x = "Hour of day (EST)",
y = "% of tweets",
color = "") +
scale_color_brewer(palette="Set1")
View(tweets)
View(tweets)
filter(year(with_tz(created_at, "EST"))>2014, year(with_tz(created_at, "EST"))<2017)
tweets %>% filter(year(with_tz(created_at, "EST"))>2014, year(with_tz(created_at, "EST"))<2017)
test <- tweets %>% filter(year(with_tz(created_at, "EST"))>2014, year(with_tz(created_at, "EST"))<2017)
View(test)
tweets %>%
count(source,
quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
ggplot(aes(source, n, fill = quoted)) +
geom_bar(stat = "identity", position = "dodge") +
theme(title=element_text(size=18),axis.title=element_text(size=18), axis.text=element_text(size=18),legend.text=element_text(size=18)) +
labs(x = "", y = "Number of tweets", fill = "") +
ggtitle('Whether tweets start with a quotation mark (")') +
scale_fill_brewer(palette="Dark2")
tweets <- tweets %>%
filter(created_at < "2017-03-01" & created_at > "2015-06-01")
# Preparation of the dataset
# L notation ensures that the number is stored as an integer not a double
tweets.source <- tweets %>%
mutate(fromiPhone = ifelse(source=="iPhone", 0L, 1L)) %>%
select(source, fromiPhone, text, created_at)
View(tweets.source)
tweets <- tweets %>%
filter(created_at < "2017-03-01" & created_at > "2015-06-01")
# Preparation of the dataset
# L notation ensures that the number is stored as an integer not a double
tweets.source <- tweets %>%
mutate(fromiPhone = ifelse(source=="iPhone", 1L, 0L)) %>%
select(source, fromiPhone, text, created_at)
# Preparation of the dataset
# L notation ensures that the number is stored as an integer not a double
tweets.source <- tweets %>%
mutate(fromiPhone = ifelse(source=="iPhone", 1L, 0L)) %>%
select(source, fromiPhone, text, created_at)
# Break down tweets into one word per line
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets.source %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "http")) %>% # replaciong links by "http"
mutate(text = str_replace_all(text, "'", "")) %>%
mutate(text = str_replace_all(text, "badly", "bad")) %>% #manual stemming
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]")) %>%
count(word, fromiPhone, created_at, source)
tweet_words = data.frame(tweet_words)
Android_iPhone_ratios <- tweet_words %>%
group_by(word) %>%
filter(sum(n) >= 40) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate(ID.iPhone = ifelse(is.na(iPhone/sum(iPhone)),0,iPhone/sum(iPhone))) %>%
mutate(ID.Android = ifelse(is.na(Android/sum(Android)),0,Android/sum(Android))) %>%
group_by(word) %>%
summarise(ID.iPhone = sum(ID.iPhone), ID.Android = sum(ID.Android)) %>%
ungroup() %>%
mutate(logratio = ifelse(ID.iPhone==0,10,ifelse(ID.Android==0,-10,log2( ID.Android / ID.iPhone)))) %>%
arrange(desc(logratio))
Android_iPhone_ratios  %>%
filter(logratio > 0) %>%
top_n(20, logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='red', show.legend=FALSE) +
coord_flip() +
ylim(0,10) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
Android_iPhone_ratios  %>%
filter(logratio < 0) %>%
top_n(20, -logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='lightblue', show.legend=FALSE) +
coord_flip() +
ylim(-10,0) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
tweets.trump <- tweets %>%
mutate(TrumpWrote = ifelse(source=="iPhone", 0L, 1L)) %>%
select(TrumpWrote, text, created_at)
# Writing data file
write.csv(tweets.trump, "trump_tweets.csv")
View(tweets.trump)
# Preparation of the dataset
# L notation ensures that the number is stored as an integer not a double
tweets.source <- tweets %>%
mutate(fromiPhone = ifelse(source=="iPhone", 0L, 1L)) %>%
select(source, fromiPhone, text, created_at)
# Break down tweets into one word per line
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets.source %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "http")) %>% # replaciong links by "http"
mutate(text = str_replace_all(text, "'", "")) %>%
mutate(text = str_replace_all(text, "badly", "bad")) %>% #manual stemming
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]")) %>%
count(word, fromiPhone, created_at, source)
tweet_words = data.frame(tweet_words)
Android_iPhone_ratios <- tweet_words %>%
group_by(word) %>%
filter(sum(n) >= 40) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate(ID.iPhone = ifelse(is.na(iPhone/sum(iPhone)),0,iPhone/sum(iPhone))) %>%
mutate(ID.Android = ifelse(is.na(Android/sum(Android)),0,Android/sum(Android))) %>%
group_by(word) %>%
summarise(ID.iPhone = sum(ID.iPhone), ID.Android = sum(ID.Android)) %>%
ungroup() %>%
mutate(logratio = ifelse(ID.iPhone==0,10,ifelse(ID.Android==0,-10,log2( ID.Android / ID.iPhone)))) %>%
arrange(desc(logratio))
Android_iPhone_ratios  %>%
filter(logratio > 0) %>%
top_n(20, logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='red', show.legend=FALSE) +
coord_flip() +
ylim(0,10) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
Android_iPhone_ratios  %>%
filter(logratio < 0) %>%
top_n(20, -logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='lightblue', show.legend=FALSE) +
coord_flip() +
ylim(-10,0) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
tweets.trump <- tweets %>%
mutate(TrumpWrote = ifelse(source=="iPhone", 0L, 1L)) %>%
select(TrumpWrote, text, created_at)
# Writing data file
write.csv(tweets.trump, "trump_tweets.csv")
View(tweets.trump)
# Preparation of the dataset
# L notation ensures that the number is stored as an integer not a double
tweets.source <- tweets %>%
mutate(fromiPhone = ifelse(source=="iPhone", 1L, 0L)) %>%
select(source, fromiPhone, text, created_at)
# Break down tweets into one word per line
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets.source %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "http")) %>% # replaciong links by "http"
mutate(text = str_replace_all(text, "'", "")) %>%
mutate(text = str_replace_all(text, "badly", "bad")) %>% #manual stemming
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]")) %>%
count(word, fromiPhone, created_at, source)
tweet_words = data.frame(tweet_words)
Android_iPhone_ratios <- tweet_words %>%
group_by(word) %>%
filter(sum(n) >= 40) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate(ID.iPhone = ifelse(is.na(iPhone/sum(iPhone)),0,iPhone/sum(iPhone))) %>%
mutate(ID.Android = ifelse(is.na(Android/sum(Android)),0,Android/sum(Android))) %>%
group_by(word) %>%
summarise(ID.iPhone = sum(ID.iPhone), ID.Android = sum(ID.Android)) %>%
ungroup() %>%
mutate(logratio = ifelse(ID.iPhone==0,10,ifelse(ID.Android==0,-10,log2( ID.Android / ID.iPhone)))) %>%
arrange(desc(logratio))
Android_iPhone_ratios  %>%
filter(logratio > 0) %>%
top_n(20, logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='red', show.legend=FALSE) +
coord_flip() +
ylim(0,10) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
Android_iPhone_ratios  %>%
filter(logratio < 0) %>%
top_n(20, -logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='lightblue', show.legend=FALSE) +
coord_flip() +
ylim(-10,0) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
tweets.trump <- tweets %>%
mutate(TrumpWrote = ifelse(source=="iPhone", 0L, 1L)) %>%
select(TrumpWrote, text, created_at)
# Writing data file
write.csv(tweets.trump, "trump_tweets.csv")
Android_iPhone_ratios  %>%
filter(logratio > 0) %>%
top_n(20, logratio) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_bar(stat = "identity", fill='red', show.legend=FALSE) +
coord_flip() +
ylim(0,10) +
ylab("Android/iPhone log ratio") +
xlab("") +
theme(axis.text.x=element_text(size=18), axis.text.y=element_text(size=18))
library(mlr)
library(OOBCurve)
library(tidyverse)
library(lubridate)
library(dplyr)
library(purrr)
library(ggplot2)
library(scales)
library(stringr)
library(tidytext)
library(glmnet)
library(tm)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(data.table)
library(ROCR)
library(leaps)
library(xgboost)
#################
#Analysis can begin here
#################
#read in data
trump.tweets<-read.csv("data/trump_tweets.csv")
setwd("~/Google Drive/Colab Notebooks/R/4.trump_tweet")
#################
#Analysis can begin here
#################
#read in data
trump.tweets<-read.csv("data/trump_tweets.csv")
#################
#Analysis can begin here
#################
#read in data
trump.tweets<-read.csv("data\trump_tweets.csv")
#################
#Analysis can begin here
#################
#read in data
trump.tweets<-read.csv("data/trump_tweets.csv")
#################
#Analysis can begin here
#################
#read in data
trump.tweets<-read.csv("trump_tweets.csv")
### Definition and restriction of corpus of words
# Definition of corpus of words
corpus = Corpus(VectorSource(tweets.trump$text))
# 1. Everything in lower case
corpus = tm_map(corpus, tolower)
# 1. Everything in lower case
corpus = tm_map(corpus, tolower)
# 2. Transform "https://link" into "https link" to make sure https is a word
f <- content_transformer(function(x, oldtext,newtext) gsub(oldtext, newtext, x))
corpus <- tm_map(corpus, f, "https://", "http ")
# 3. Remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# 4. Remove stop words and other particular words manually
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords, c("realdonaldtrump", "donaldtrump"))
# 5. Stemming
corpus = tm_map(corpus, stemDocument)
View(corpus)
# Fraction of dataset used in the analysis
pct.text <- sum(as.matrix(sparse)) / sum(as.matrix(frequencies))
